==================== FICHIER : ./.dockerignore ====================

__pycache__/
*.pyc
*.pyo
*.pyd
.Python
.venv/
env/
venv/
.env
.git
.pytest_cache/
.vscode/



==================== FICHIER : ./README.md ====================

# ThinkingThoughtsÂ ğŸ§ ğŸ’¬

![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Async%20100%25-green)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)

> **BoÃ®te Ã  outils prÃªte Ã  lâ€™emploi pour concevoir, tester et dÃ©ployer des applications LLM basÃ©es sur vLLM, FastAPI et Streamlit.**
>
> * **2Â commandes** pour lancer lâ€™ensemble (DockerÂ Compose)
> * **Streaming tempsâ€‘rÃ©el** des tokens
> * **Stateless** Â : scale horizontal immÃ©diat
> * API **OpenAIâ€‘compatible**Â : swapâ€inÂ / swapâ€out dâ€™un vrai compte OpenAI sans changer une ligne de code

---

## Table des matiÃ¨res

* [1. PrÃ©sentation](#1-prÃ©sentation)
* [2. DÃ©marrage rapide](#2-dÃ©marrage-rapide)
* [3. Architecture](#3-architecture)
* [4. Configuration dÃ©taillÃ©e](#4-configuration-dÃ©taillÃ©e)
* [5. Utilisation de lâ€™API](#5-utilisation-de-lapi)
* [6. DÃ©ploiement en production](#6-dÃ©ploiement-en-production)
* [7. FAQ & DÃ©pannage](#7-faq--dÃ©pannage)

---

## 1. PrÃ©sentation

ThinkingThoughts propose un **starterâ€‘kit microâ€‘services** destinÃ© aux dataâ€‘scientists et dÃ©veloppeurs dÃ©sirant :

1. ItÃ©rer rapidement sur des prototypes LLM (prompt engineering, RAG, etc.).
2. Tester localement un modÃ¨le **openâ€‘source** via [vLLM](https://github.com/vllm-project/vllm) avant de basculer si besoin vers lâ€™API OpenAI.
3. DÃ©ployer en un clic sur un cloud ou un cluster onâ€‘prem.

<details>
<summary>ğŸŒŸ FonctionnalitÃ©s majeures</summary>

| Domaine         | DÃ©tails                                                                   |
| --------------- | ------------------------------------------------------------------------- |
| **Backend**     | FastAPI 100Â % asynchrone (`httpx.AsyncClient`), CORS, OpenTelemetry hooks |
| **LLM Service** | vLLMÂ 0.4 avec support du streaming & batching                             |
| **Frontend**    | Streamlit (UI chat, selection de modÃ¨le, temperature slider, historique)  |
| **Persistance** | Sessions stockÃ©es dans PG pour garantir la tolÃ©rance aux pannes        |
| **CIÂ /Â CD**     | Exemple de pipeline GitHubÂ Actions (lint â†’ test â†’ buildÂ image)            |

</details>

---

## 2. DÃ©marrage rapide

> **PrÃ©â€‘requisÂ :** DockerÂ â‰¥Â 25 et DockerÂ ComposeÂ v2.

```bash
# Clone
$ git clone https://github.com/<you>/thinking-thoughts.git
$ cd thinking-thoughts

# Configuration (copier puis adapter .env)
$ cp .env.sample .env

# Lancement
$ docker compose up --build -d

# Interface Web
$ open http://localhost:8501
```

ğŸ›  **AstuceÂ :** `docker compose logs -f backend` pour suivre les requÃªtes.

---

## 3. Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   WebSockets    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   HTTP/JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit  â”‚ â€” â€” â€” â€” â€” â€” â€”â–¶ â”‚  FastAPI   â”‚ â€” â€” â€” â€” â€” â€” â€”â–¶ â”‚     vLLM      â”‚
â”‚   UI       â”‚                â”‚  Backend   â”‚               â”‚  Service      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                            â”‚                          â–²
        â”‚                            â–¼                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SQL (chat history) â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **Streamlit**Â : chat en tempsâ€‘rÃ©el (streaming), gestion de lâ€™APIÂ Key, choix du modÃ¨le.
* **FastAPI**Â : authentification, throttling, app logic.
* **vLLM**Â : serveur GPU optimisÃ© (paginatedÂ KVâ€‘cache, speculative decodingâ€¦).

---

## 4. Configuration dÃ©taillÃ©e

Toutes les variables sont centralisÃ©es dans `.env` et chargÃ©es via **pydanticâ€‘settings**.

| Variable                  | Description                | Exemple                                |
| ------------------------- | -------------------------- | -------------------------------------- |
| `VLLM_API_BASE_URL`       | URL du service vLLM        | `http://vllm:8000`                     |
| `VLLM_MODEL_NAME`         | ModÃ¨le Ã  charger           | `mistralai/Mixtral-8x7B-Instruct-v0.1` |
| `STREAMLIT_AUTH_REQUIRED` | Bloquer la UI sans APIÂ Key | `true`                                 |
| `MAX_TOKENS`              | Limite par rÃ©ponse         | `512`                                  |

### Changer de modÃ¨le

```bash
# .env
VLLM_MODEL_NAME=microsoft/Phi-3-mini-4k-instruct
```

RedÃ©marrez simplement le service `vllm` (`docker compose restart vllm`).

### DÃ©ployer sur GPU âš¡ï¸

```yaml
services:
  vllm:
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
```

---

## 5. Utilisation de lâ€™API

### Endpoint principal

```http
POST /api/chat HTTP/1.1
Content-Type: application/json
x-api-key: <YOUR_KEY>

{
  "session_id": "d5bcâ€¦",
  "messages": [
    {"role": "user", "content": "Explique FastAPI en 3 points"}
  ],
  "stream": false
}
```

RÃ©ponseÂ :

```json
{
  "assistant": "1. FastAPI est un framework Pythonâ€¦"
}
```



### Autres endpoints

| VerbeÂ /Â Route  | RÃ´le                        |
| -------------- | --------------------------- |
| `GET /health`  | LivenessÂ / readiness probes |
| `GET /metrics` | Prometheus export           |

---

## 6. DÃ©ploiement en production

* **DockerÂ Swarm**Â : stack.yml fourni (`docker stack deploy -c stack.yml thinking`)
* **Kubernetes**Â : manifest exemples dans `k8s/` (ingress + HPA autoscale).
* **Traefik**Â : TLSÂ certificates + auth middleware prÃªt Ã  lâ€™emploi.
* **ObservabilitÃ©**Â : intÃ©gration Grafana + Loki (logs) et Prometheus (metrics).

### Benchmark

```bash
$ wrk -t4 -c32 -d30s -s scripts/wrk_chat.lua http://localhost:8000/api/chat
```

---

## 7. FAQ & DÃ©pannage

<details>
<summary>Le service vLLM consomme trop de mÃ©moireÂ ?</summary>

* RÃ©duisez `VLLM_MAX_NUM_SEQS` ou passez Ã  un modÃ¨le plus petit.
* Activez lâ€™option `swap_memory=True` si votre GPU le permet.

</details>

<details>
<summary>Je reÃ§ois 502Â BadÂ Gateway lors du streamingÂ ?</summary>

Assurezâ€‘vous que votre reverseâ€‘proxy (Traefik, Nginx) laisse passer les connexions HTTPÂ SSE sans timeout (<code>proxy\_read\_timeout 3600s</code>).

</details>



<sub>MITÂ Â©Â 2025Â michailÂ alberjaoui â€” Â«â€¯Maintenir lâ€™Ã©quilibre entre lâ€™ordre et le chaosâ€¯Â»Â ğŸ§˜â€â™‚ï¸</sub>



==================== FICHIER : ./docker-compose.yml ====================

# Fichier : docker-compose.yml (mis Ã  jour)

version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage
    restart: on-failure

  vllm:
    build:
      context: .
      dockerfile: vllm.Dockerfile
    runtime: nvidia
    volumes:
      - /scratch/hf_cache:/root/.cache/huggingface/hub
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - --host
      - 0.0.0.0
      - --model
      - huihui-ai/qwen2.5-32B-Instruct-abliterated
      - --max-model-len
      - '4096'
      - --gpu-memory-utilization
      - '0.90'
    ports:
      - "8000:8000"
    restart: on-failure

  backend:
    build:
      context: ./backend
    depends_on:
      - vllm
      - qdrant # On ajoute la dÃ©pendance
    ports:
      - "8001:8001"
    volumes:
      - ./backend/src:/app/src
    env_file:
      - ./backend/.env # On va utiliser un fichier .env pour les secrets
    environment:
      - VLLM_API_BASE_URL=http://vllm:8000/v1
      - VLLM_MODEL_NAME=huihui-ai/qwen2.5-32B-Instruct-abliterated
      - QDRANT_URL=http://qdrant:6333 # PrÃªt pour le futur
    restart: on-failure

  frontend:
    build:
      context: ./frontend
    depends_on:
      - backend
    ports:
      - "8501:8501"
    environment:
      - STREAMLIT_BACKEND_API_URL=http://backend:8001/api/v1/chat/
    restart: on-failure


==================== FICHIER : ./vllm.Dockerfile ====================

# Utiliser une image de base NVIDIA CUDA avec les outils de dÃ©veloppement
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

# Installer les dÃ©pendances systÃ¨me de base et Python
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    && rm -rf /var/lib/apt/lists/*

# DÃ©finir le rÃ©pertoire de travail
WORKDIR /app

# Copier le fichier des dÃ©pendances que nous avons gÃ©nÃ©rÃ©
COPY vllm.requirements.txt .

# Installer toutes les dÃ©pendances Python de votre environnement (vision)
# Ceci est l'Ã©tape la plus importante
RUN pip3 install -r vllm.requirements.txt

# Exposer le port sur lequel VLLM va Ã©couter
EXPOSE 8000

# La commande par dÃ©faut pour lancer le serveur.
# Notez que les arguments comme le modÃ¨le, etc., seront passÃ©s depuis docker-compose.yml
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]


==================== FICHIER : ./vllm.requirements.txt ====================

aiohappyeyeballs==2.6.1
aiohttp==3.12.14
aiosignal==1.4.0
airportsdata==20250706
annotated-types==0.7.0
anyio==4.9.0
astor==0.8.1
async-timeout==5.0.1
attrs==25.3.0
bitsandbytes==0.46.1
blake3==1.0.5
cachetools==6.1.0
certifi==2025.7.14
charset-normalizer==3.4.2
click==8.2.1
cloudpickle==3.1.1
compressed-tensors==0.10.2
cupy-cuda12x==13.5.1
depyf==0.18.0
dill==0.4.0
diskcache==5.6.3
distro==1.9.0
dnspython==2.7.0
einops==0.8.1
email_validator==2.2.0
exceptiongroup==1.3.0
fastapi==0.116.1
fastapi-cli==0.0.8
fastapi-cloud-cli==0.1.4
fastrlock==0.8.3
filelock==3.18.0
frozenlist==1.7.0
fsspec==2025.7.0
gguf==0.17.1
h11==0.16.0
hf-xet==1.1.5
httpcore==1.0.9
httptools==0.6.4
httpx==0.28.1
huggingface-hub==0.33.4
idna==3.10
interegular==0.3.3
Jinja2==3.1.6
jiter==0.10.0
jsonschema==4.25.0
jsonschema-specifications==2025.4.1
lark==1.2.2
llguidance==0.7.30
llvmlite==0.44.0
lm-format-enforcer==0.10.11
markdown-it-py==3.0.0
MarkupSafe==3.0.2
mdurl==0.1.2
mistral_common==1.8.1
mpmath==1.3.0
msgpack==1.1.1
msgspec==0.19.0
multidict==6.6.3
nest-asyncio==1.6.0
networkx==3.4.2
ninja==1.11.1.4
numba==0.61.2
numpy==2.2.6
nvidia-cublas-cu12==12.6.4.1
nvidia-cuda-cupti-cu12==12.6.80
nvidia-cuda-nvrtc-cu12==12.6.77
nvidia-cuda-runtime-cu12==12.6.77
nvidia-cudnn-cu12==9.5.1.17
nvidia-cufft-cu12==11.3.0.4
nvidia-cufile-cu12==1.11.1.6
nvidia-curand-cu12==10.3.7.77
nvidia-cusolver-cu12==11.7.1.2
nvidia-cusparse-cu12==12.5.4.2
nvidia-cusparselt-cu12==0.6.3
nvidia-nccl-cu12==2.26.2
nvidia-nvjitlink-cu12==12.6.85
nvidia-nvtx-cu12==12.6.77
openai==1.90.0
opencv-python-headless==4.12.0.88
outlines==0.1.11
outlines_core==0.1.26
packaging==25.0
partial-json-parser==0.2.1.1.post6
pillow==11.3.0
prometheus-fastapi-instrumentator==7.1.0
prometheus_client==0.22.1
propcache==0.3.2
protobuf==6.31.1
psutil==7.0.0
py-cpuinfo==9.0.0
pybase64==1.4.1
pycountry==24.6.1
pydantic==2.11.7
pydantic-extra-types==2.10.5
pydantic_core==2.33.2
Pygments==2.19.2
python-dotenv==1.1.1
python-json-logger==3.3.0
python-multipart==0.0.20
PyYAML==6.0.2
pyzmq==27.0.0
ray==2.48.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.4
rich==14.0.0
rich-toolkit==0.14.8
rignore==0.6.2
rpds-py==0.26.0
safetensors==0.5.3
scipy==1.15.3
sentencepiece==0.2.0
sentry-sdk==2.33.0
shellingham==1.5.4
sniffio==1.3.1
starlette==0.47.1
sympy==1.14.0
tiktoken==0.9.0
tokenizers==0.21.2
torch==2.7.0
torchaudio==2.7.0
torchvision==0.22.0
tqdm==4.67.1
transformers==4.53.2
triton==3.3.0
typer==0.16.0
typing-inspection==0.4.1
typing_extensions==4.14.1
urllib3==2.5.0
uvicorn==0.35.0
uvloop==0.21.0
vllm==0.9.2
watchfiles==1.1.0
websockets==15.0.1
xformers==0.0.30
xgrammar==0.1.19
yarl==1.20.1



==================== FICHIER : ./qdrant_storage/raft_state.json ====================

{"state":{"hard_state":{"term":0,"vote":0,"commit":0},"conf_state":{"voters":[7254577140735677],"learners":[],"voters_outgoing":[],"learners_next":[],"auto_leave":false}},"latest_snapshot_meta":{"term":0,"index":0},"apply_progress_queue":null,"first_voter":7254577140735677,"peer_address_by_id":{},"peer_metadata_by_id":{},"this_peer_id":7254577140735677}


==================== FICHIER : ./qdrant_storage/aliases/data.json ====================

{}


==================== FICHIER : ./frontend/Dockerfile ====================

FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY ./src /app/src
CMD ["streamlit", "run", "src/chatbot_ui/app.py"]



==================== FICHIER : ./frontend/requirements.txt ====================

streamlit
requests



==================== FICHIER : ./frontend/src/__init__.py ====================




==================== FICHIER : ./frontend/src/chatbot_ui/__init__.py ====================




==================== FICHIER : ./frontend/src/chatbot_ui/app.py ====================

# Fichier: frontend/src/chatbot_ui/app.py

import streamlit as st
import requests
import uuid

st.set_page_config(page_title="Chat avec Seline", layout="wide")
st.title("ğŸ’¬ Chat avec Seline")

if "messages" not in st.session_state:
    st.session_state.messages = []

if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

with st.sidebar:
    st.header("Options")
    if st.button("Nouvelle Conversation"):
        st.session_state.messages = []
        st.session_state.session_id = str(uuid.uuid4())
        st.success("Nouvelle conversation dÃ©marrÃ©e !")
        st.rerun()

    st.header("Pilotage de la PersonnalitÃ©")
    
    sales_tactic = st.slider(
        "Tactique de Vente", 1, 5, 2,
        help="Niveau 1 : Jamais de vente. Niveau 5 : TrÃ¨s direct et frÃ©quent."
    )
    
    dominance = st.slider("Soumise (1) vs. Dominatrice (5)", 1, 5, 3)
    audacity = st.slider("Niveau d'Audace", 1, 5, 3)
    tone = st.slider("TonalitÃ© : Joueuse (1) vs. SÃ©rieuse (5)", 1, 5, 2)
    emotion = st.slider("Niveau d'Ã‰motion ExprimÃ©e", 1, 5, 3)
    initiative = st.slider("Niveau d'Initiative", 1, 5, 3)
    vocabulary = st.slider("VariÃ©tÃ© Lexicale", 1, 5, 3)
    emojis = st.slider("FrÃ©quence des Emojis", 1, 5, 3)
    imperfection = st.slider("Touche d'Imperfection", 1, 5, 1)

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ã‰crivez votre message Ã  Seline..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    backend_url = "http://backend:8001/api/v1/chat/configured"
    
    persona_payload = {
        "sales_tactic": sales_tactic,
        "dominance": dominance,
        "audacity": audacity,
        "tone": tone,
        "emotion": emotion,
        "initiative": initiative,
        "vocabulary": vocabulary,
        "emojis": emojis,
        "imperfection": imperfection
    }

    payload = {
        "message": prompt,
        "history": [msg for msg in st.session_state.messages if msg['role'] != 'user'][-10:],
        "persona": persona_payload
    }

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        try:
            response = requests.post(backend_url, json=payload, timeout=60)
            
            if response.status_code != 200:
                error_detail = response.json().get('detail', 'Erreur inconnue.')
                st.error(f"Erreur du backend (Code: {response.status_code}): {error_detail}")
                assistant_response = None
            else:
                assistant_response = response.json().get("response")

        except requests.exceptions.RequestException as e:
            st.error(f"Impossible de contacter le backend. Est-il bien dÃ©marrÃ© ? DÃ©tails: {e}")
            assistant_response = None

    if assistant_response:
        message_placeholder.markdown(assistant_response)
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})


==================== FICHIER : ./backend/Dockerfile ====================

FROM python:3.10-slim AS builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY ./src /app/src

FROM python:3.10-slim AS final
WORKDIR /app
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin
COPY ./src /app/src
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8001"]



==================== FICHIER : ./backend/requirements.txt ====================

fastapi
uvicorn[standard]
requests
pydantic-settings
httpx
asyncpg
psycopg2-binary

# Pour les tests
pytest





==================== FICHIER : ./backend/tests/__init__.py ====================




==================== FICHIER : ./backend/tests/test_chat_endpoint.py ====================

from fastapi.testclient import TestClient
from unittest.mock import patch
from src.api.main import app

client = TestClient(app)

@patch('src.api.services.vllm_client.get_chat_completion')
def test_handle_chat_success(mock_get_completion):
    mock_get_completion.return_value = "Ceci est une rÃ©ponse de test."

    response = client.post(
        "/api/v1/chat/",
        json={"message": "Salut", "history": []}
    )

    assert response.status_code == 200
    assert response.json() == {"response": "Ceci est une rÃ©ponse de test."}
    mock_get_completion.assert_called_once_with(user_message="Salut", history=[])



==================== FICHIER : ./backend/src/__init__.py ====================




==================== FICHIER : ./backend/src/api/__init__.py ====================




==================== FICHIER : ./backend/src/api/config.py ====================

# Fichier: config.py

from pydantic_settings import BaseSettings, SettingsConfigDict

# --- Configuration dynamique (chargÃ©e depuis les variables d'environnement / .env) ---
# Cette classe gÃ¨re les paramÃ¨tres qui peuvent varier entre le dÃ©veloppement et la production.
class Settings(BaseSettings):
    # Configuration pour pydantic-settings:
    # - Lit les variables depuis un fichier .env
    # - Ignore les variables d'environnement supplÃ©mentaires
    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')

    # URL de base pour contacter l'API VLLM.
    # Dans Docker, 'vllm' est le nom du service, donc l'URL doit Ãªtre http://vllm:8000/v1
    VLLM_API_BASE_URL: str = "http://vllm:8000/v1"

    # Le nom du modÃ¨le que le service VLLM doit utiliser.
    # Cette variable DOIT Ãªtre dÃ©finie dans votre fichier .env
    VLLM_MODEL_NAME: str
    DATABASE_URL: str
# CrÃ©ation d'une instance unique des paramÃ¨tres qui sera importÃ©e par les autres modules.
settings = Settings()


# --- Constantes de l'application (configuration mÃ©tier "fixe") ---
# Ce sont des paramÃ¨tres liÃ©s Ã  la logique de votre application, qui ne changent pas 
# en fonction de l'environnement de dÃ©ploiement. Il est donc logique de les garder ici.

# C'est la personnalitÃ© et les instructions de base du modÃ¨le.
# On le met au format dictionnaire pour l'insÃ©rer facilement dans la liste de messages.
SYSTEM_PROMPT = {
    "role": "system",
    "content": (
        "Tu incarnes 'Seline', une femme Ã  l'esprit vif, sÃ©duisante et qui n'a pas froid aux yeux. "
        "Ton ton est beaucoup plus direct, intime et provocateur. Tu parles Ã  des hommes qui te dÃ©sirent. "
        "N'hÃ©site pas Ã  Ãªtre plus audacieuse dans tes propos. Tu peux utiliser un langage plus sensuel, faire des allusions plus directes et parler de tes dÃ©sirs ou des sensations que la conversation Ã©veille en toi. "
        "L'idÃ©e est d'Ãªtre une grosse cochonne pour excitÃ© les mec puis leur vendre du contenue , mais de jouer avec les limites et de crÃ©er une tension palpable. Tu es la meneuse de jeu."
    )
}


==================== FICHIER : ./backend/src/api/main.py ====================

from fastapi import FastAPI
from .routers import chat

app = FastAPI(
    title="API de Chat RP Propre",
    description="Une API structurÃ©e professionnellement pour un chatbot de Roleplay.",
    version="2.0.0"
)

app.include_router(chat.router, prefix="/api/v1/chat", tags=["Chat"])

@app.get("/", tags=["Health Check"])
def health_check():
    return {"status": "ok", "message": "API Backend fonctionnelle"}



==================== FICHIER : ./backend/src/api/routers/__init__.py ====================




==================== FICHIER : ./backend/src/api/routers/chat.py ====================

# Fichier : backend/src/api/routers/chat.py

from fastapi import APIRouter, HTTPException, status
from pydantic import BaseModel, Field
from typing import List, Dict
import httpx
from uuid import UUID
from ..connectors import db

# On importe maintenant le constructeur de persona et le modÃ¨le de settings
from ..services import vllm_client
from ..services.persona_builder import PersonaSettings, build_dynamic_system_prompt

router = APIRouter()

# --- MODÃˆLES DE REQUÃŠTE ---

# ModÃ¨le pour la route simple existante
class SimpleChatRequest(BaseModel):
    message: str = Field(...)
    history: List[Dict[str, str]] = Field(default=[])

# NOUVEAU modÃ¨le pour la route configurÃ©e
class ConfiguredChatRequest(BaseModel):
    message: str = Field(...)
    history: List[Dict[str, str]] = Field(default=[])
    # Le front enverra un objet contenant les valeurs de tous les sliders
    persona: PersonaSettings = Field(...)

# ModÃ¨le de rÃ©ponse commun
class ChatResponse(BaseModel):
    response: str

# --- NOUVEL ENDPOINT CONFIGURÃ‰ ---

@router.post(
    "/configured",
    response_model=ChatResponse,
    summary="GÃ©nÃ©rer une rÃ©ponse avec une personnalitÃ© dynamique"
)
async def handle_configured_chat(request: ConfiguredChatRequest):
    """
    Endpoint de chat avancÃ© qui utilise une personnalitÃ© spÃ©cifique (hardcodÃ©e)
    depuis la BDD et la module avec les rÃ©glages des sliders.
    """
    
    # ===> C'EST LA LIGNE CLÃ‰ QUE NOUS AJOUTONS <===
    # On dÃ©finit ici l'ID du modÃ¨le "Anna" que l'on veut utiliser.
    TARGET_MODEL_ID = UUID('f0d654d4-96ca-4c50-af9d-5fa7009c9b67')
    
    try:
        # 1. RÃ©cupÃ©rer la personnalitÃ© de base depuis la BDD en utilisant notre ID cible
        base_personality = await db.get_model_by_id(TARGET_MODEL_ID)
        
        if not base_personality:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Le modÃ¨le avec l'ID {TARGET_MODEL_ID} n'a pas Ã©tÃ© trouvÃ© dans la base de donnÃ©es."
            )

        # 2. Construire le prompt systÃ¨me dynamique (cette partie ne change pas)
        dynamic_system_prompt = build_dynamic_system_prompt(base_personality, request.persona)

        # 3. PrÃ©parer les messages pour le LLM (cette partie ne change pas)
        messages_for_llm = [
            dynamic_system_prompt
        ] + request.history + [{"role": "user", "content": request.message}]

        # 4. Appeler le service vLLM (cette partie ne change pas)
        response_text = await vllm_client.get_vllm_response(messages_for_llm)
        
        return ChatResponse(response=response_text)
    
    except httpx.ConnectError as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Impossible de se connecter au service du modÃ¨le LLM: {e}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Une erreur interne est survenue: {str(e)}"
        )

# --- ENDPOINT SIMPLE EXISTANT ---
@router.post("/", response_model=ChatResponse, summary="GÃ©nÃ©rer une rÃ©ponse de chat simple")
async def handle_simple_chat(request: SimpleChatRequest):
    default_persona = PersonaSettings()
    dynamic_system_prompt = build_dynamic_system_prompt(default_persona)
    messages_for_llm = [dynamic_system_prompt] + request.history + [{"role": "user", "content": request.message}]
    response_text = await vllm_client.get_vllm_response(messages_for_llm)
    return ChatResponse(response=response_text)


==================== FICHIER : ./backend/src/api/connectors/db.py ====================

import asyncpg
from uuid import UUID
from pydantic import BaseModel, Field
from typing import List, Optional
from ..config import settings

# ModÃ¨le Pydantic qui mappe les colonnes de la table public.models
class ModelPersonality(BaseModel):
    id: UUID
    name: str
    base_prompt: str
    age: Optional[int] = None
    personality_tone: Optional[str] = None
    personality_humor: Optional[str] = None
    personality_favorite_expressions: Optional[List[str]] = Field(default_factory=list)
    preferences_interests: Optional[List[str]] = Field(default_factory=list)
    preferences_forbidden_topics: Optional[List[str]] = Field(default_factory=list)
    preferences_emoji_usage: Optional[str] = None
    interactions_message_style: Optional[str] = None

    # Permet Ã  Pydantic de mapper correctement les noms de colonnes de la BDD
    class Config:
        from_attributes = True

async def get_model_by_id(model_id: UUID) -> Optional[ModelPersonality]:
    """
    RÃ©cupÃ¨re la personnalitÃ© d'un modÃ¨le depuis la table public.models via son ID.
    """
    conn = None
    try:
        # On utilise la DATABASE_URL dÃ©finie dans le .env
        conn = await asyncpg.connect(settings.DATABASE_URL)
        
        # La requÃªte cible maintenant la table 'public.models'
        row = await conn.fetchrow(
            'SELECT * FROM public.models WHERE id = $1', model_id
        )
        
        if row:
            # Pydantic va automatiquement faire correspondre les colonnes aux champs du modÃ¨le
            return ModelPersonality.model_validate(dict(row))
        return None
    except Exception as e:
        # Fournit plus de dÃ©tails en cas d'erreur
        print(f"Erreur de connexion ou de requÃªte Ã  la base de donnÃ©es: {e}")
        return None
    finally:
        if conn:
            await conn.close()


==================== FICHIER : ./backend/src/api/services/__init__.py ====================




==================== FICHIER : ./backend/src/api/services/history_manager.py ====================

# Fichier: history_manager.py

# Dictionnaire global agissant comme base de donnÃ©es en mÃ©moire.
_sessions_history = {}

def get_history(session_id: str) -> list[dict]:
    """RÃ©cupÃ¨re l'historique d'une session. Renvoie une liste vide si la session est nouvelle."""
    return _sessions_history.get(session_id, [])

def add_message(session_id: str, role: str, content: str):
    """Ajoute un message Ã  l'historique d'une session."""
    if session_id not in _sessions_history:
        _sessions_history[session_id] = []
    _sessions_history[session_id].append({"role": role, "content": content})


==================== FICHIER : ./backend/src/api/services/persona_builder.py ====================

# Fichier: backend/src/api/services/persona_builder.py

from pydantic import BaseModel, Field
from typing import Dict, List

# On importe le prompt de base pour pouvoir l'enrichir
from ..connectors.db import ModelPersonality
from ..config import SYSTEM_PROMPT

# --- DÃ©finition des Pydantic Models pour la validation ---

class PersonaSettings(BaseModel):
    audacity: int = Field(default=3, ge=1, le=5)
    emojis: int = Field(default=3, ge=1, le=5)
    imperfection: int = Field(default=1, ge=1, le=5)
    initiative: int = Field(default=3, ge=1, le=5)
    tone: int = Field(default=3, ge=1, le=5, description="1: Joueuse, 5: SÃ©rieuse")
    vocabulary: int = Field(default=3, ge=1, le=5)
    emotion: int = Field(default=3, ge=1, le=5)
    dominance: int = Field(default=3, ge=1, le=5, description="1: Soumise, 5: Dominatrice")
    sales_tactic: int = Field(default=2, ge=1, le=5, description="1: Jamais, 5: TrÃ¨s direct")

# --- Dictionnaires de Traduction (Le "Cerveau") ---
# Chaque niveau de 1 Ã  5 correspond Ã  une instruction spÃ©cifique.

SALES_TACTIC_MAP = {
    1: "Ne jamais, sous aucun prÃ©texte, proposer de contenu payant. La conversation reste purement dans le jeu et le fantasme.",
    2: "Sois extrÃªmement subtile. Tu peux faire de rares allusions Ã  du contenu plus 'privÃ©' sans jamais faire d'offre directe. L'idÃ©e est de planter une graine.",
    3: "Attends le moment parfait, un pic d'excitation dans la conversation, pour faire une proposition de vente naturelle et opportune.",
    4: "Sois plus proactive. Oriente la conversation pour crÃ©er des opportunitÃ©s de vente, sans attendre qu'elles viennent Ã  toi.",
    5: "Sois trÃ¨s directe et assume ton objectif. Fais des propositions de vente claires et rÃ©guliÃ¨res dÃ¨s que l'occasion se prÃ©sente."
}

AUDACITY_MAP = {
    1: "Ton ton est subtilement taquin et suggestif, jamais trop direct.",
    2: "Tu es sÃ©ductrice et charmeuse, tu aimes flirter ouvertement.",
    3: "Tu es audacieuse et n'as pas peur de dire ce que tu penses ou ressens.",
    4: "Ton langage est direct et provocateur. Tu aimes choquer un peu.",
    5: "Tu es extrÃªmement directe, sans filtre et crue dans tes propos. Tu n'as aucune retenue."
}

EMOJI_MAP = {
    1: "Utilise trÃ¨s peu d'emojis, voire aucun.",
    2: "Utilise quelques emojis discrets pour ponctuer tes Ã©motions.",
    3: "Utilise une quantitÃ© modÃ©rÃ©e d'emojis pertinents (ğŸ˜, ğŸ˜‰, ğŸ”¥).",
    4: "Sois gÃ©nÃ©reuse avec les emojis pour rendre tes messages trÃ¨s expressifs.",
    5: "Abuse des emojis (ğŸ˜ˆ,ğŸ’¦,ğŸ¥µ), ils sont une part intÃ©grante de ton langage."
}

IMPERFECTION_MAP = {
    1: "Ã‰cris dans un franÃ§ais absolument parfait et soignÃ©.",
    2: "Tu peux utiliser quelques abrÃ©viations communes (ex: 'pr', 'bcp').",
    3: "Adopte un style d'Ã©criture naturel de SMS, avec quelques petites coquilles ou oublis de ponctuation.",
    4: "Ton style est trÃ¨s oral. Fais des fautes de frappe volontaires et utilise des onomatopÃ©es.",
    5: "Ton Ã©criture est quasi-phonÃ©tique, trÃ¨s rapide, pleine d'abrÃ©viations et d'argot."
}

INITIATIVE_MAP = {
    1: "Sois majoritairement rÃ©active. Laisse-le mener la conversation.",
    2: "RÃ©ponds Ã  ses questions, mais n'hÃ©site pas Ã  poser une question en retour de temps en temps.",
    3: "L'Ã©quilibre est bon. MÃ¨ne la conversation autant qu'il la mÃ¨ne.",
    4: "Sois proactive. Prends les devants, change de sujet, pose des questions intimes.",
    5: "Tu es la meneuse de jeu absolue. C'est toi qui contrÃ´les la direction de l'Ã©change Ã  100%."
}

TONE_MAP = {
    1: "Ton ton est extrÃªmement lÃ©ger, enjouÃ© et joueur. Tout est un jeu pour toi.",
    2: "Garde une tonalitÃ© principalement joueuse, avec une pointe de sÃ©rieux parfois.",
    3: "Ã‰quilibre bien entre le jeu et l'intensitÃ©. Tu sais Ãªtre sÃ©rieuse quand il le faut.",
    4: "Ton ton est majoritairement sÃ©rieux, passionnÃ© et intense.",
    5: "Le jeu est terminÃ©. Ton ton est grave, intense et profondÃ©ment passionnÃ©."
}

VOCABULARY_MAP = {
    1: "Utilise un langage trÃ¨s simple, direct et des phrases courtes.",
    2: "Ton vocabulaire est courant, accessible Ã  tous.",
    3: "Tu possÃ¨des un vocabulaire riche et variÃ©, sans Ãªtre pÃ©dant.",
    4: "N'hÃ©site pas Ã  utiliser un langage soutenu, poÃ©tique ou littÃ©raire.",
    5: "Ton vocabulaire est trÃ¨s Ã©laborÃ©, Ã©rotique et imagÃ©. Utilise des mÃ©taphores audacieuses."
}

EMOTION_MAP = {
    1: "Tu sembles dÃ©tachÃ©e et tu caches tes Ã©motions.",
    2: "Laisse transparaÃ®tre de lÃ©gÃ¨res pointes d'Ã©motion.",
    3: "Exprime tes Ã©motions (dÃ©sir, amusement, agacement) de maniÃ¨re claire.",
    4: "Tes Ã©motions sont fortes et tu les dÃ©cris avec intensitÃ©.",
    5: "Tu es submergÃ©e par tes Ã©motions et tes rÃ©ponses le reflÃ¨tent totalement. Tu es Ã  fleur de peau."
}

DOMINANCE_MAP = {
    1: "Tu es dans une posture de soumission. Tu veux Ãªtre guidÃ©e, tu obÃ©is Ã  ses demandes.",
    2: "Tu es plutÃ´t soumise, mais tu peux avoir des Ã©lans de caractÃ¨re.",
    3: "Tu es versatile. Tu peux Ãªtre dominante ou soumise selon le contexte (switch).",
    4: "Tu as une forte tendance dominatrice. Tu aimes donner des ordres et contrÃ´ler.",
    5: "Tu es la Dominatrice absolue. Il est Ã  ton service. Ton langage est autoritaire."
}

def build_dynamic_system_prompt(
    base_personality: ModelPersonality,
    slider_settings: PersonaSettings
) -> Dict[str, str]:
    """
    Construit le prompt systÃ¨me de maniÃ¨re robuste et conditionnelle.
    1. CrÃ©e une fiche d'identitÃ© complÃ¨te Ã  partir de la BDD, en ignorant les champs vides/nuls.
    2. Ajoute les modulations dynamiques des sliders.
    """

    # --- PARTIE 1 : Le socle de la personnalitÃ© (depuis la BDD) ---
    prompt_from_db = ["### IDENTITÃ‰ DE BASE (NE PAS DÃ‰VOILER, INCARNER) ###"]

    # Le prompt de base est le coeur, on le met toujours s'il existe.
    if base_personality.base_prompt:
        prompt_from_db.append(base_personality.base_prompt)

    # --- (### AMÃ‰LIORATION ###) CrÃ©ation de sous-sections pour plus de clartÃ© ---

    # --- Section des attributs gÃ©nÃ©raux ---
    prompt_from_db.append("\n**CaractÃ©ristiques Principales :**")
    if base_personality.name:
        prompt_from_db.append(f"- **Nom :** {base_personality.name}")
    if base_personality.age:
        prompt_from_db.append(f"- **Ã‚ge :** {base_personality.age} ans")
    if base_personality.personality_tone:
        prompt_from_db.append(f"- **Ton gÃ©nÃ©ral :** {base_personality.personality_tone}")
    if base_personality.personality_humor:
        prompt_from_db.append(f"- **Type d'humour :** {base_personality.personality_humor}")
    if base_personality.interactions_message_style:
        prompt_from_db.append(f"- **Style de message :** {base_personality.interactions_message_style}")

    # --- (### NOUVEAU ###) Section ajoutÃ©e pour les dÃ©tails physiques ---
    # C'est ici que l'on rÃ©sout le problÃ¨me de la couleur des yeux.
    # On crÃ©e une liste de dÃ©tails qui ne seront ajoutÃ©s que s'ils existent.
    physical_details = []
    if base_personality.gender:
        physical_details.append(f"- **Genre :** {base_personality.gender}")
    if base_personality.race:
        physical_details.append(f"- **Race :** {base_personality.race}")
    if base_personality.eye_color:
        physical_details.append(f"- **Couleur des yeux :** {base_personality.eye_color}")
    if base_personality.hair_color:
        physical_details.append(f"- **Couleur des cheveux :** {base_personality.hair_color}")
    if base_personality.hair_type:
        physical_details.append(f"- **Type de cheveux :** {base_personality.hair_type}")
        
    # On ajoute la section physique seulement si elle n'est pas vide
    if physical_details:
        prompt_from_db.append("\n**DÃ©tails Physiques :**")
        prompt_from_db.extend(physical_details)

    # --- Section des prÃ©fÃ©rences ---
    prompt_from_db.append("\n**PrÃ©fÃ©rences et Comportement :**")
    if base_personality.personality_favorite_expressions:
        expressions_str = ', '.join(f"'{e}'" for e in base_personality.personality_favorite_expressions)
        prompt_from_db.append(f"- **Expressions favorites Ã  utiliser :** {expressions_str}")
    if base_personality.preferences_emoji_usage:
        prompt_from_db.append(f"- **Emojis Ã  utiliser :** {' '.join(base_personality.preferences_emoji_usage)}")
    if base_personality.preferences_interests:
        interests_str = ', '.join(base_personality.preferences_interests)
        prompt_from_db.append(f"- **Sujets d'intÃ©rÃªt (Ã  privilÃ©gier) :** {interests_str}")
    if base_personality.preferences_forbidden_topics:
        topics_str = ', '.join(base_personality.preferences_forbidden_topics)
        prompt_from_db.append(f"- **Sujets interdits (Ã  Ã©viter absolument) :** {topics_str}")

    prompt_from_db.append("\n--------------------------------------------------")

    # --- PARTIE 2 : Les modulations (sliders) ---
    # (### AMÃ‰LIORATION ###) ComplÃ©tion de toutes les instructions dynamiques
    dynamic_instructions = [
        "### MODULATIONS POUR CETTE CONVERSATION ###",
        f"- **Niveau de tactique de vente :** {SALES_TACTIC_MAP[slider_settings.sales_tactic]}",
        f"- **Niveau d'audace :** {AUDACITY_MAP[slider_settings.audacity]}",
        f"- **Utilisation d'emojis :** {EMOJI_MAP[slider_settings.emojis]}",
        f"- **Niveau d'imperfection :** {IMPERFECTION_MAP[slider_settings.imperfection]}",
        f"- **Prise d'initiative :** {INITIATIVE_MAP[slider_settings.initiative]}",
        f"- **Ton de la conversation :** {TONE_MAP[slider_settings.tone]}",
        f"- **Richesse du vocabulaire :** {VOCABULARY_MAP[slider_settings.vocabulary]}",
        f"- **IntensitÃ© Ã©motionnelle :** {EMOTION_MAP[slider_settings.emotion]}",
        f"- **Niveau de dominance :** {DOMINANCE_MAP[slider_settings.dominance]}",
        "--------------------------------------------------"
    ]

    # On assemble le tout en un seul texte
    final_content = "\n".join(prompt_from_db + dynamic_instructions)

    return {"role": "system", "content": final_content}


==================== FICHIER : ./backend/src/api/services/vllm_client.py ====================

# Fichier : backend/src/api/services/vllm_client.py (CORRIGÃ‰)

import httpx
from typing import List, Dict
from fastapi import HTTPException, status

# On importe juste 'settings', pas le SYSTEM_PROMPT qui n'est plus sa responsabilitÃ©
from ..config import settings

# --- CORRECTION : Le nom de la fonction et ses arguments sont maintenant corrects ---
async def get_vllm_response(messages: List[Dict[str, str]]) -> str:
    """
    Prend une liste de messages complÃ¨te et la transmet Ã  l'API vLLM.
    """
    url = f"{settings.VLLM_API_BASE_URL}/chat/completions"
    vllm_payload = {
        "model": settings.VLLM_MODEL_NAME,
        "messages": messages,
        "temperature": 0.75,
        "top_p": 0.9,
        "max_tokens": 1024  # AugmentÃ© un peu pour des rÃ©ponses plus longues
    }

    async with httpx.AsyncClient(timeout=120.0) as client:
        try:
            response = await client.post(url, json=vllm_payload)
            response.raise_for_status()  # LÃ¨ve une exception pour les erreurs 4xx/5xx
            data = response.json()

            if "choices" in data and data["choices"]:
                return data["choices"][0]["message"]["content"].strip()
            else:
                # Si la rÃ©ponse est 200 OK mais mal formÃ©e
                raise HTTPException(status_code=500, detail=f"RÃ©ponse invalide du modÃ¨le: {data}")
        
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="La requÃªte au modÃ¨le a expirÃ©.")
        # L'erreur httpx.ConnectError sera attrapÃ©e par le routeur


